{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02677b-bcc9-4a71-ac71-14adf6b7ab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90c18a3-1b71-42b4-ba8a-96bdb1d16a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace as sn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from rich import print\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94275d-5713-4929-a653-68bfef241c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37dc8ff-0433-40c2-a23a-fc696be8842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556f8e6-0f93-484e-bb11-4dccfed6c806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e4d8e-af96-4cd9-87b8-265edffe8890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88efe6-6d53-4489-882d-e1d0939089a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be7b02-456f-431d-bbb4-aad73439cd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0739850-34d1-4f17-97a5-b4206739471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rainbow import Rainbow, reset_noise\n",
    "from env_wrappers import create_env\n",
    "from utils import get_mean_ep_length\n",
    "from sb3_logger import configure_logger, WandbOutputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc4ccb6-3975-41ab-8579-bc13c6783b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from old_rainbow.common import argp\n",
    "from old_rainbow.common.rainbow import Rainbow as old_Rainbow\n",
    "from old_rainbow.common.env_wrappers import create_env as old_create_env #, BASE_FPS_ATARI, BASE_FPS_PROCGEN\n",
    "from old_rainbow.common.utils import LinearSchedule\n",
    "from old_rainbow.common.utils import get_mean_ep_length as old_get_mean_ep_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da9aa74-c846-47d6-9a0c-c2b57e728ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_named(net, old_named, new_named):\n",
    "    same = True\n",
    "    for old_p, new_p in zip(old_named, new_named):\n",
    "        if old_p[1].grad is not None and not torch.allclose(old_p[1].grad, new_p[1].grad):\n",
    "            same = False\n",
    "            print(f'{net}: {old_p[0]}.grad != {new_p[0]}.grad: {(old_p[1].grad - new_p[1].grad).abs().max().item()}')\n",
    "        if not torch.allclose(old_p[1], new_p[1]):\n",
    "            same = False\n",
    "            print(f'{net}: {old_p[0]} != {new_p[0]}: {(old_p[1] - new_p[1]).abs().max().item()}')\n",
    "    return same\n",
    "\n",
    "def same_net(net, old_net, new_net):\n",
    "    same_params = same_named(net, old_net.named_parameters(), new_net.named_parameters())\n",
    "    same_buffers = same_named(net, old_net.named_buffers(), new_net.named_buffers())\n",
    "    return same_params and same_buffers\n",
    "\n",
    "def same_model(old_rainbow, new_rainbow):\n",
    "    same_current = same_net('current', old_rainbow.q_policy, new_rainbow.q_net)\n",
    "    same_target = same_net('target', old_rainbow.q_target, new_rainbow.q_net_target)\n",
    "    return same_current and same_target\n",
    "\n",
    "def same_replays(old_rainbow, new_rainbow):\n",
    "    if old_rainbow.buffer.size != new_rainbow.replay_buffer.size():\n",
    "        print(f'Old replay buffer size {old_rainbow.buffer.size} != new size {new_rainbow.replay_buffer.size()}')\n",
    "        return False\n",
    "    if hasattr(old_rainbow.buffer, 'max_priority') != hasattr(new_rainbow.replay_buffer, 'max_priority'):\n",
    "        print('Using different replay buffers (Uniform/Priority)')\n",
    "        return False\n",
    "    priority = hasattr(old_rainbow.buffer, 'max_priority')\n",
    "    \n",
    "    o_obs, o_nobs, o_a, o_r, o_d = list(zip(*old_rainbow.buffer.data[:old_rainbow.buffer.size]))\n",
    "    o_obs = np.array([np.array(o) for o in o_obs]) #convert lazy frames to numpy arrays\n",
    "    o_nobs = np.array([np.array(o) for o in o_nobs])\n",
    "    o_a = torch.stack(o_a).detach().cpu().numpy()\n",
    "    o_r = torch.concat(o_r).detach().cpu().numpy()\n",
    "    o_d = torch.concat(o_d).detach().cpu().numpy()\n",
    "    \n",
    "    data = new_rainbow.replay_buffer.observations, new_rainbow.replay_buffer.next_observations, new_rainbow.replay_buffer.actions, new_rainbow.replay_buffer.rewards, new_rainbow.replay_buffer.dones\n",
    "    n_obs, n_nobs, n_a, n_r, n_d = map(lambda d: d[:new_rainbow.replay_buffer.size()], data)\n",
    "    n_obs = np.array([np.array(o) for o in n_obs]) #convert lazy frames to numpy arrays\n",
    "    n_nobs = np.array([np.array(o) for o in n_nobs])\n",
    "    \n",
    "    for name, old_v, new_v in zip(('observations', 'next_observations', 'actions', 'rewards', 'dones'), (o_obs, o_nobs, o_a, o_r, o_d), (n_obs, n_nobs, n_a, n_r, n_d)):\n",
    "        if not (old_v == new_v).all():\n",
    "            print(f'{name} are not the same')\n",
    "            return False\n",
    "    \n",
    "    if priority:\n",
    "        if not np.allclose(old_rainbow.buffer.max_priority, new_rainbow.replay_buffer.max_priority):\n",
    "            print('Max priorities are not close')\n",
    "            return False\n",
    "        if not np.allclose(old_rainbow.buffer.priority_sum, new_rainbow.replay_buffer.priority_sum):\n",
    "            print('Priority sums are not close')\n",
    "            return False\n",
    "        if not np.allclose(old_rainbow.buffer.priority_min, new_rainbow.replay_buffer.priority_min):\n",
    "            print('Priority mins are not close')\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcaf426e-9a25-4512-8ded-c03c27c8e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80496549-c3f6-42b4-9ba7-05486c50f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_env(args, old=False):\n",
    "    set_random(args)\n",
    "    \n",
    "    create = old_create_env if old else create_env\n",
    "    ep_length = old_get_mean_ep_length if old else get_mean_ep_length\n",
    "    \n",
    "    decorr_steps = None\n",
    "    if args.env_name == 'gym:Breakout':\n",
    "        decorr_steps = 160 // args.parallel_envs\n",
    "    if args.decorr and not args.env_name.startswith('procgen:') and decorr_steps is None:\n",
    "        decorr_steps = ep_length(args) // args.parallel_envs\n",
    "    env = create(args, decorr_steps=decorr_steps)\n",
    "    #states = env.reset()\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cae331-3791-47f3-8c33-255d7a96a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sb3(args):\n",
    "    env = set_up_env(args)\n",
    "    \n",
    "    model = Rainbow('CnnPolicy',\n",
    "                    env,\n",
    "                    buffer_size=args.buffer_size,\n",
    "                    batch_size=args.batch_size,\n",
    "                    learning_starts=args.burnin,\n",
    "                    target_update_interval=args.sync_dqn_target_every,\n",
    "                    gradient_steps=args.gradient_steps,\n",
    "                    exploration_fraction=args.exploration_fraction,\n",
    "                    policy_kwargs={\n",
    "                        'noisy_linear': args.noisy_linear, \n",
    "                        'linear_kwargs': args.linear_kwargs, \n",
    "                        'optimizer_class': args.optimizer_class,\n",
    "                        'optimizer_kwargs': args.optimizer_kwargs,\n",
    "                        'features_extractor_kwargs': {'model_size': args.model_size}\n",
    "                    })\n",
    "    \n",
    "    return model, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1b61ca-60d0-45a0-be56-2b9f7f1ab699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_default_args(args):\n",
    "    args.prioritized_er_beta0 = 0.45\n",
    "    args.prioritized_er_time = args.training_frames\n",
    "    args.use_amp = False\n",
    "    args.network_arch = f'impala_large:{args.model_size}'\n",
    "    args.spectral_norm = 'all'\n",
    "    args.noisy_dqn = args.noisy_linear\n",
    "    if args.noisy_dqn:\n",
    "        args.noisy_sigma0 = args.linear_kwargs['sigma_0']\n",
    "        args.init_eps = 0.002\n",
    "        args.final_eps = 0.0\n",
    "        args.eps_decay_frames = max(int(0.002 * args.training_frames), 1)\n",
    "    else:\n",
    "        args.init_eps = 1.\n",
    "        args.final_eps = 0.01\n",
    "        #args.eps_decay_frames = max(int(0.05 * args.training_frames), 1)\n",
    "    args.double_dqn = True\n",
    "    args.prioritized_er = True\n",
    "    args.n_step = 3\n",
    "    args.max_grad_norm = 10\n",
    "    args.lr = 0.00025\n",
    "    args.adam_eps = 0.005/args.batch_size if args.adam_eps is None else args.adam_eps\n",
    "    args.lr_decay_steps = None\n",
    "    args.loss_fn = 'huber'\n",
    "    args.train_count = args.gradient_steps\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da5005c-bcce-45d6-8a8a-41d8cef46770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_old(args):\n",
    "    env = set_up_env(args, old=True)\n",
    "    \n",
    "    args = add_default_args(args)\n",
    "    \n",
    "    rainbow = old_Rainbow(env, args)\n",
    "    \n",
    "    return rainbow, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef063870-0cb9-4ddd-b64e-99841a713153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_initialization(args):\n",
    "    new_model, _ = initialize_sb3(args)\n",
    "    old_model, _ = initialize_old(args)\n",
    "    \n",
    "    try:\n",
    "        assert same_model(old_model, new_model)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return new_model, old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "795b3093-4056-411e-9efe-d153a7a345be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sb3(args):\n",
    "    model, env = initialize_sb3(args)\n",
    "    \n",
    "    model.learn(args.training_frames, progress_bar=True)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1514ece8-436e-43c7-a30f-6bb3d1adad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_old(args):\n",
    "    rainbow, old_env = initialize_old(args)\n",
    "    \n",
    "    states = old_env.reset()\n",
    "    \n",
    "    eps_schedule = LinearSchedule(0, initial_value=args.init_eps, final_value=args.final_eps, decay_time=args.eps_decay_frames)\n",
    "    per_beta_schedule = LinearSchedule(0, initial_value=args.prioritized_er_beta0, final_value=1.0, decay_time=args.prioritized_er_time)\n",
    "    \n",
    "    episode_count = 0\n",
    "    returns = deque(maxlen=100)\n",
    "    discounted_returns = deque(maxlen=10)\n",
    "    losses = deque(maxlen=10)\n",
    "    q_values = deque(maxlen=10)\n",
    "    grad_norms = deque(maxlen=10)\n",
    "    iter_times = deque(maxlen=10)\n",
    "    reward_density = 0\n",
    "    \n",
    "    returns_all = []\n",
    "    q_values_all = []\n",
    "    \n",
    "    t = trange(0, args.training_frames + 1, args.parallel_envs)\n",
    "    for game_frame in t:\n",
    "        iter_start = time.time()\n",
    "        eps = eps_schedule(game_frame)\n",
    "        per_beta = per_beta_schedule(game_frame)\n",
    "    \n",
    "        # reset the noisy-nets noise in the policy\n",
    "        if args.noisy_dqn:\n",
    "            rainbow.reset_noise(rainbow.q_policy)\n",
    "    \n",
    "        # compute actions to take in all parallel envs, asynchronously start environment step\n",
    "        actions = rainbow.act(states, eps)\n",
    "        old_env.step_async(actions)\n",
    "    \n",
    "        # if training has started, perform args.train_count training steps, each on a batch of size args.batch_size\n",
    "        if rainbow.buffer.burnedin:\n",
    "            print('Trained')\n",
    "            for train_iter in range(args.train_count):\n",
    "                if args.noisy_dqn and train_iter > 0: rainbow.reset_noise(rainbow.q_policy)\n",
    "                q, loss, grad_norm = rainbow.train(args.batch_size, beta=per_beta)\n",
    "                losses.append(loss)\n",
    "                grad_norms.append(grad_norm)\n",
    "                q_values.append(q)\n",
    "                q_values_all.append((game_frame, q))\n",
    "    \n",
    "        # copy the Q-policy weights over to the Q-target net\n",
    "        # (see also https://github.com/spragunr/deep_q_rl/blob/master/deep_q_rl/launcher.py#L155)\n",
    "        if game_frame % args.sync_dqn_target_every == 0 and rainbow.buffer.burnedin:\n",
    "            rainbow.sync_Q_target()\n",
    "    \n",
    "        # block until environments are ready, then collect transitions and add them to the replay buffer\n",
    "        next_states, rewards, dones, infos = old_env.step_wait()\n",
    "        \n",
    "        #transitions.append((states, actions, rewards, dones))\n",
    "        \n",
    "        for state, action, reward, done, j in zip(states, actions, rewards, dones, range(args.parallel_envs)):\n",
    "            reward_density = 0.999 * reward_density + (1 - 0.999) * (reward != 0)\n",
    "            rainbow.buffer.put(state, action, reward, done, j=j)\n",
    "        states = next_states\n",
    "    \n",
    "        # if any of the envs finished an episode, log stats to wandb\n",
    "        for info, j in zip(infos, range(args.parallel_envs)):\n",
    "            if 'episode_metrics' in info.keys():\n",
    "                episode_metrics = info['episode_metrics']\n",
    "                returns.append(episode_metrics['return'])\n",
    "                returns_all.append((game_frame, episode_metrics['return']))\n",
    "                discounted_returns.append(episode_metrics['discounted_return'])\n",
    "    \n",
    "                episode_count += 1\n",
    "                \n",
    "        #if game_frame % (10_000-(10_000 % args.parallel_envs)) == 0:\n",
    "        #    print(f' [{game_frame:>8} frames, {episode_count:>5} episodes] running average return = {np.mean(returns)}')\n",
    "        #    torch.cuda.empty_cache()\n",
    "    \n",
    "        iter_times.append(time.time() - iter_start)\n",
    "        t.set_description(f' [{game_frame:>8} frames, {episode_count:>5} episodes]', refresh=False)\n",
    "        \n",
    "    old_env.close()\n",
    "    \n",
    "    return rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db057d9b-9e74-47cb-908e-e59bcc01c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pre_optimization(args):\n",
    "    args.training_frames = args.burnin - 5 # stop procedure before training starts\n",
    "    \n",
    "    new_model = train_sb3(args)\n",
    "    old_model = train_old(args)\n",
    "    \n",
    "    try:\n",
    "        assert same_model(old_model, new_model)\n",
    "        assert same_replays(old_model, new_model)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return new_model, old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27349e0e-3a3e-4203-9824-8f0262654773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_first_update(args):\n",
    "    args.training_frames = ((args.burnin // args.parallel_envs) + 5) * args.parallel_envs # stop procedure after first update\n",
    "    args.gradient_steps = 1 # only do one update, otherwise non-determinism gets amplified and prevents proper comparison\n",
    "    \n",
    "    new_model = train_sb3(args)\n",
    "    old_model = train_old(args)\n",
    "    \n",
    "    try:\n",
    "        assert same_model(old_model, new_model)\n",
    "        assert same_replays(old_model, new_model)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return new_model, old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4801e8a-af15-4286-9d1f-cc170545ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(env_name='gym:Breakout',\n",
    "                 parallel_envs=64,\n",
    "                 subproc_vecenv=False,\n",
    "                 time_limit=108_000,\n",
    "                 frame_stack=4,\n",
    "                 frame_skip=4,\n",
    "                 grayscale=True,\n",
    "                 gamma=0.99,\n",
    "                 resolution=(84, 84),\n",
    "                 save_dir='tmp',\n",
    "                 record_every=60*50,\n",
    "                 decorr=True,\n",
    "                 seed=3605)\n",
    "\n",
    "args.burnin = 500 #100_000\n",
    "args.buffer_size = 2**12 #2**19\n",
    "args.batch_size = 256\n",
    "args.sync_dqn_target_every = 320 #32_000\n",
    "args.training_frames = 64*11 #1000 #2_000_000\n",
    "\n",
    "args.eps_decay_frames = 700\n",
    "args.exploration_fraction = args.eps_decay_frames / (args.training_frames + args.parallel_envs)\n",
    "\n",
    "args.noisy_linear = False\n",
    "args.linear_kwargs = {'sigma_0': 0.5} if args.noisy_linear else {}\n",
    "    \n",
    "args.adam_eps = None\n",
    "args.optimizer_kwargs = {'eps': args.adam_eps}\n",
    "args.model_size = 2\n",
    "args.gradient_steps = 2\n",
    "\n",
    "from cbp import CBP\n",
    "args.optimizer_class = CBP\n",
    "args.optimizer_kwargs |= {'m':500, 'rho':10**-5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd3a1d-acbd-4eb5-b04b-bd2c45ff2bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4647e69b-05d0-49e1-96f5-2970410fa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model, old_model = compare_initialization(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad7c4d2f-2628-4510-9082-f3c7faf5552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model, old_model = compare_pre_optimization(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e523c8ff-981b-4237-ba62-3cb8eced3cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a690dfda7ca49bc997209e298bd4f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [     704 frames,     1 episodes]:  92%|██████████████████████████████████████████▍   | 12/13 [00:01<00:00,  7.83it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [     768 frames,     1 episodes]: 100%|██████████████████████████████████████████████| 13/13 [00:02<00:00,  5.67it/s]\n"
     ]
    }
   ],
   "source": [
    "new_model, old_model = compare_first_update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f0de6d3-cf79-42f5-a170-8de9134b1f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(torch.numel, new_model.q_net.features_extractor.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2d14991-2de0-4672-b1dc-820761c2792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098949"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(torch.numel, new_model.q_net.dueling.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f2286-6aae-4cff-946d-3937b95ae446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbp",
   "language": "python",
   "name": "conda-env-cbp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
