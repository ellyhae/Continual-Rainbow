#TODO specify the Hydra config setup and all individual parameters

defaults:
  - _self_
  #- env: atari
  - random: seed1
  - algorithm: baseline
#  - override hydra/launcher: joblib

hydra:
  sweep:
    dir: hydra_output/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.runtime.choices.algorithm}/${hydra.runtime.choices.random}
  job:
    chdir: True
#  launcher:
#    n_jobs: 4

name: ${hydra:runtime.choices.algorithm}_${hydra:runtime.choices.random}

seed_name: ${hydra:runtime.choices.random}

model_dir: ./
cbp_dir: ./cbp
cbp_log_freq: 100

progress_bar: False

algorithm:
  training_frames: 10_000_000

  tensorboard_log: ${hydra:runtime.cwd}/${hydra:sweep.dir}/tensorboard

  settings:
    policy: CnnPolicy

    learning_starts: 80_000
    buffer_size: 1_048_576 #2**20
    batch_size: 256
    target_update_interval: 32_000
    gradient_steps: 2
    replay_buffer_kwargs:
      n_step: 3
    prioritized_er_beta0_initial: 0.45
    gamma: 0.99
    
    learning_rate: 0.00025
    loss_fn: 'huber'
    max_grad_norm: 10

    eps_decay_frames: 500_000  # used to calculate exploration_fraction # hard to determine what number would be appropriate without noisy_linear

    policy_kwargs:
      noisy_linear: False
      linear_kwargs: {}
      optimizer_kwargs:
        betas: [0.9, 0.999]
        eps: null  # will be set in the code

checkpoint_dir: ./checkpoints/
checkpoint_freq: 15625   # 1_000_000 / 64 (n_envs)

eval:
  # eval during training
  evalcallback: True
  eval_freq: 15625   # 1_000_000 / 64 (n_envs)
  n_eval_episodes: 10
  dir: ./eval
  best_model_dir: ./

wandb:
  project: Rainbow
  name: ${name}
  group: ${hydra:runtime.choices.algorithm}
  mode: online
  dir: ${hydra:runtime.cwd}/${hydra:sweep.dir}
  monitor_gym: False # auto-upload the videos of agents playing the game
wandb_gradient_save_freq: 1000