# @package _global_
defaults:
  - /env@_global_: control

algorithm:
  training_frames: 50000 #?

  settings:
    policy: MlpPolicy

    learning_starts: 500
    buffer_size: 50000 #2**20
    batch_size: 128
    target_update_interval: 100
    gradient_steps: 2
    replay_buffer_kwargs:
      n_step: 3
    prioritized_er_beta0_initial: 0.45
    prioritized_er_beta0_final: 0.51
    gamma: 0.99
    
    learning_rate: 0.001
    loss_fn: 'huber'
    max_grad_norm: 10

    eps_decay_frames: 30000  # used to calculate exploration_fraction # hard to determine what number would be appropriate without noisy_linear

    policy_kwargs:
      noisy_linear: False
      linear_kwargs: {}

      optimizer_class: Adam
      optimizer_kwargs:
        betas: [0.9, 0.999]
        eps: 3.125e-4

eval:
  eval_freq: 500

#settings:
#  policy_kwargs:
#    optimizer_class: Adam